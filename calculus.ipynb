{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzcx4HcnGwux"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Partial Derivatives and Gradients**  \n",
        "-------------------------------------\n",
        "\n",
        "Thus far, we have been differentiating functions of just one variable. In deep learning, we also need to work with functions of *many* variables. We briefly introduce notions of the derivative that apply to such *multivariate* functions.\n",
        "\n",
        "Let $y = f(x_1, x_2, \\dots, x_n)$ be a function with $n$ variables. The *partial derivative* of y with respect to its $i^{th}$ parameter $x_i$ is\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i}\n",
        "\\;=\\;\n",
        "\\lim_{h \\to 0}\n",
        "\\frac{f(x_1,\\dots,x_{i-1},x_i + h,\\,x_{i+1},\\dots,x_n) \\;-\\; f(x_1,\\dots,x_i,\\dots,x_n)}{h}.\n",
        "\\tag{2.4.6}\n",
        "$$\n",
        "\n",
        "To calculate $\\frac{\\partial y}{\\partial x_i}$, we treat all other $x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_{n}$ as constants and calculate the derivative of $y$ with respect to $x_i$. The following notational conventions for partial derivatives are all common and all mean the same thing:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i}\n",
        "=\n",
        "\\frac{\\partial f}{\\partial x_i}\n",
        "=\n",
        "\\partial_{x_i} f\n",
        "=\n",
        "\\partial_i f\n",
        "=\n",
        "f_{x_i}\n",
        "=\n",
        "f_i\n",
        "=\n",
        "D_i f\n",
        "=\n",
        "D_{x_i} f.\n",
        "\\tag{2.4.7}\n",
        "$$\n",
        "\n",
        "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain a vector that is called the gradient of the function. Suppose that the input of function $f:\\mathbb R^n\\to\\mathbb R$ is an $n$-dimensional vector $\\mathbf x=[x_1,\\dots,x_n]^T$ and the output is a scalar. The gradient of the function $f$ with respect to **x** is a vector of $n$ partial derivatives:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf x}f(\\mathbf x)\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\partial_{x_1}f(\\mathbf x)\\\\\n",
        "\\partial_{x_2}f(\\mathbf x)\\\\\n",
        "\\vdots\\\\\n",
        "\\partial_{x_n}f(\\mathbf x)\n",
        "\\end{bmatrix}.\n",
        "\\tag{2.4.8}\n",
        "$$\n",
        "\n",
        "When there is no ambiguity $\\nabla_{\\mathbf x} f(\\mathbf x)$ is typically replaced by $\\nabla f(\\mathbf x)$. The following rules come in handy for differentiating multivariate functions:\n",
        "\n",
        "- For all $A\\in\\mathbb R^{m\\times n}$ we have $\\nabla_{\\mathbf x}(A\\mathbf x)=A^T$ and $\\nabla_{\\mathbf x}(\\mathbf x^T A)=A$.  \n",
        "- For square matrices $A\\in\\mathbb R^{n\\times n}: \\nabla_{\\mathbf x}(\\mathbf x^T A\\mathbf x)=(A+A^T)\\mathbf x$, in particular $\\nabla_{\\mathbf x}\\|\\mathbf x\\|^2=2\\mathbf x$.\n",
        "\n",
        "Similarly, for any matrix $X$, we have\n",
        "\n",
        "$$\n",
        "\\nabla_X\\|X\\|_F^2=2X.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4. Chain Rule**\n",
        "\n",
        "### **4.1 Introduction to the Chain Rule**\n",
        "\n",
        "The Chain Rule is a mathematical tool used to compute the derivative of composite functions.  \n",
        "In deep learning, we deal with complex, nested functions across multiple layers. The Chain Rule enables us to compute gradients for optimization.\n",
        "\n",
        "There are two main cases:\n",
        "\n",
        "- **Single-variable functions**: $y = f(g(x))$\n",
        "- **Multivariable functions**: $y = f(\\mathbf{u})$, where $\\mathbf{u} = g(\\mathbf{x})$\n",
        "\n",
        "### **4.2 Chain Rule for Single-Variable Functions**\n",
        "\n",
        "**Formula:** If $y = f(u)$ and $u = g(x)$, and both $f$ and $g$ are differentiable at their respective points, then:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
        "$$\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "- $\\frac{dy}{dx}$: Rate of change of $y$ with respect to $x$\n",
        "- $\\frac{dy}{du}$: Rate of change of $y$ with respect to $u$\n",
        "- $\\frac{du}{dx}$: Rate of change of $u$ with respect to $x$\n",
        "\n",
        "The Chain Rule \"chains\" these derivatives together.\n",
        "\n",
        "**Example: Find the derivative of the function $y = \\sin(x^2)$.**\n",
        "\n",
        "**Step 1: Identify the composition**\n",
        "\n",
        "Let\n",
        "$$\n",
        "u = x^2, \\quad \\text{then} \\quad y = \\sin(u).\n",
        "$$\n",
        "\n",
        "**Step 2: Apply the Chain Rule**\n",
        "\n",
        "By the Chain Rule:\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
        "\n",
        "**Step 3: Compute each part**\n",
        "\n",
        "- $\\frac{dy}{du} = \\cos(u)$  \n",
        "- $\\frac{du}{dx} = 2x$\n",
        "\n",
        "**Step 4: Substitute back**\n",
        "\n",
        "$$\\frac{dy}{dx} = \\cos(u) \\cdot 2x = \\cos(x^2) \\cdot 2x$$\n",
        "\n",
        "### **4.3 Chain Rule for Multivariable Functions**\n",
        "\n",
        "**Formula:**  \n",
        "If $y = f(\\mathbf{u})$, where $\\mathbf{u} = (u_1, u_2, \\dots, u_m)$, and each $u_i = g_i(\\mathbf{x})$, with $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, then:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1} \\frac{\\partial u_1}{\\partial x_i} + \\frac{\\partial y}{\\partial u_2} \\frac{\\partial u_2}{\\partial x_i} + \\cdots + \\frac{\\partial y}{\\partial u_m} \\frac{\\partial u_m}{\\partial x_i}\n",
        "$$\n",
        "\n",
        "**In vector form:**\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} y = \\mathbf{A} \\nabla_{\\mathbf{u}} y\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ is a matrix containing the partial derivatives $\\frac{\\partial u_j}{\\partial x_i}$.  \n",
        "- $\\nabla_{\\mathbf{x}} y$: Gradient of $y$ with respect to $\\mathbf{x}$.  \n",
        "- $\\nabla_{\\mathbf{u}} y$: Gradient of $y$ with respect to $\\mathbf{u}$.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "- This is the generalized Chain Rule for multivariable functions.  \n",
        "- The matrix $\\mathbf{A}$ (also called the **Jacobian**) represents the relationship between $\\mathbf{u}$ and $\\mathbf{x}$.  \n",
        "- The result is a vector-matrix product, which is common in deep learning for gradient computation.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Suppose:\n",
        "\n",
        "- $y = u_1^2 + u_2^2$  \n",
        "- $u_1 = x_1 + x_2$, $u_2 = x_1 - x_2$\n",
        "\n",
        "Compute $\\frac{\\partial y}{\\partial x_1}$ and $\\frac{\\partial y}{\\partial x_2}$.\n",
        "\n",
        "**Step 1: Compute the partial derivatives**\n",
        "\n",
        "- $\\frac{\\partial y}{\\partial u_1} = 2u_1$  \n",
        "- $\\frac{\\partial y}{\\partial u_2} = 2u_2$  \n",
        "- $\\frac{\\partial u_1}{\\partial x_1} = 1$, $\\frac{\\partial u_1}{\\partial x_2} = 1$  \n",
        "- $\\frac{\\partial u_2}{\\partial x_1} = 1$, $\\frac{\\partial u_2}{\\partial x_2} = -1$\n",
        "\n",
        "**Step 2: Apply the Chain Rule**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial x_1} + \\frac{\\partial y}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial x_1} = 2u_1 \\cdot 1 + 2u_2 \\cdot 1 = 2u_1 + 2u_2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_2} = \\frac{\\partial y}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial x_2} + \\frac{\\partial y}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial x_2} = 2u_1 \\cdot 1 + 2u_2 \\cdot (-1) = 2u_1 - 2u_2\n",
        "$$\n",
        "\n",
        "**Step 3: Substitute $u_1 = x_1 + x_2$, $u_2 = x_1 - x_2$**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_1} = 2(x_1 + x_2) + 2(x_1 - x_2) = 4x_1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_2} = 2(x_1 + x_2) - 2(x_1 - x_2) = 4x_2\n",
        "$$\n",
        "\n",
        "\n",
        "**Vector Form:**\n",
        "\n",
        "- Gradient: $\\nabla_{\\mathbf{x}} y = \\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\frac{\\partial y}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 4x_1 \\\\ 4x_2 \\end{bmatrix}$  \n",
        "- Jacobian Matrix: $\\mathbf{A} = \\begin{bmatrix} \\frac{\\partial u_1}{\\partial x_1} & \\frac{\\partial u_1}{\\partial x_2} \\\\ \\frac{\\partial u_2}{\\partial x_1} & \\frac{\\partial u_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}$  \n",
        "- Gradient w.r.t. $\\mathbf{u}$: $\\nabla_{\\mathbf{u}} y = \\begin{bmatrix} 2u_1 \\\\ 2u_2 \\end{bmatrix}$\n",
        "\n",
        "**Verification:**\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} y = \\mathbf{A} \\nabla_{\\mathbf{u}} y = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} 2u_1 \\\\ 2u_2 \\end{bmatrix} = \\begin{bmatrix} 2u_1 + 2u_2 \\\\ 2u_1 - 2u_2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Substitute $u_1 = x_1 + x_2$, $u_2 = x_1 - x_2$ to get:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} y = \\begin{bmatrix} 4x_1 \\\\ 4x_2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This matches our earlier result.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
