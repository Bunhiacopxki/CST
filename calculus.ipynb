{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzcx4HcnGwux"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4.1. Derivatives and Differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Problem Statement\n",
        "\n",
        "In science, engineering and beyond, we are often faced with functions that describe how one quantity depends on another—distance as a function of time, concentration as a function of volume, cost as a function of units produced, and so on.  The **derivative** of a function provides a precise way to measure the **instantaneous rate of change** of that quantity: it tells us, at any given point, how fast the output is increasing or decreasing with respect to its input.\n",
        "\n",
        "For example, if $s(t)$ denotes the position of a car at time $t$, then the derivative $s'(t)$ is the car’s **velocity**.  If $P(t)$ models the size of a bacterial population, then $P'(t)$ gives the **growth rate**, which might accelerate or slow over time depending on nutrients or competition.  In economics, if $R(q)$ is the revenue from selling $q$ units of a product, then the derivative $R'(q)$ is the **marginal revenue**, the additional income earned by selling one more unit.\n",
        "\n",
        "Derivatives also appear in many less obvious contexts:\n",
        "\n",
        "- **Chemical kinetics**: if $C(v)$ describes the concentration of a reactant as a function of volume $v$, then $dC/dv$ measures how dilution changes concentration.  \n",
        "- **Computer networks**: if $T(t)$ is the cumulative data transferred by time $t$, then $T'(t)$ is the **throughput** or instantaneous data rate.  \n",
        "- **Geometry and graphics**: the slope of a curve $y=f(x)$ at a point, given by $f'(x)$, defines the direction of its tangent line and underlies algorithms for rendering smooth shapes.\n",
        "\n",
        "Because so many practical problems reduce to “how fast is this changing?”, mastering the basic rules of differentiation—the constant, power, exponential, logarithm, sum, product and quotient rules, and so on—allows us to turn a wide variety of real‑world questions into straightforward mechanical calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Put simply, a *derivative* is the rate of change in a function with respect to changes in its arguments. Derivatives can tell us how rapidly a loss function would increase or decrease were we to *increase* or *decrease* each parameter by an infinitesimally small amount. \n",
        "\n",
        "Formally, for functions $ f : \\mathbb{R} \\rightarrow \\mathbb{R} $, that map from scalars to scalars, the *derivative* of $ f $ at a point $ x $ is defined as\n",
        "\n",
        "$$\n",
        "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}.                \n",
        "\\tag{2.4.1}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This term on the right hand side is called a *limit* and it tells us what happens to the value of an expression as a specified variable approaches a particular value. This limit tells us what the ratio between a perturbation *h* and the change in the function value $f(x + h) - f(x) $ converges to as we shrink its size to zero.\n",
        "\n",
        "When $f'(x)$ exists, $f$ is said to be differentiable at $x$; and when $f'(x)$ exists for all $x$ on a set, e.g., the interval $[a, b]$, we say that $f$ is differentiable on this set. Not all functions are differentiable, including many that we wish to optimize, such as accuracy and the area under the receiving operating characteristic (AUC). However, because computing the derivative of the loss is a crucial step in nearly all algorithms for training deep neural networks, we often optimize a differentiable *surrogate* instead.\n",
        "\n",
        "We can interpret the derivative $f'(x)$ as the instantaneous rate of change of $f(x)$ with respect to $x$. Let’s develop some intuition with an example. Define $u = f(x) = 3x^2 - 4x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting $x = 1$, we see that $\\frac{f(x+h) - f(x)}{h}$ approaches $2$ as $h$ approaches $0$. While this experiment lacks the rigor of a mathematical proof, we can quickly see that indeed $f'(1) = 2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for h in 10.0**np.arange(-1, -6, -1):\n",
        "    print(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "h=0.10000, numerical limit=2.30000\n",
        "h=0.01000, numerical limit=2.03000\n",
        "h=0.00100, numerical limit=2.00300\n",
        "h=0.00010, numerical limit=2.00030\n",
        "h=0.00001, numerical limit=2.00003"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are several equivalent notational conventions for derivatives. Given \\( y = f(x) \\), the following expressions are equivalent:\n",
        "$$\n",
        "f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),\n",
        "\\tag{2.4.2}\n",
        "$$\n",
        "where the symbols $\\frac{d}{dx}$ and  $D$  are *differentiation operators*.  \n",
        "Below, we present the derivatives of some common functions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Constant Rule\n",
        "$$\n",
        "\\frac{d}{dx} C = 0 \\quad \\text{for any constant } C, \\tag{2.4.3} \n",
        "$$ \n",
        "**Prove:** \n",
        "By definition,\n",
        "$$\n",
        "\\frac{d}{dx} C = \\lim_{h\\to0}\\frac{C - C}{h}\n",
        "= \\lim_{h\\to0}\\frac{0}{h}\n",
        "= 0.\n",
        "$$\n",
        "Since the numerator is identically zero, the limit is zero.\n",
        "**Example.**  \n",
        "Let $f(x)=7$. Then $f'(x)=0.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "f = 7\n",
        "sp.diff(f, x)\n",
        "# → 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Power Rule\n",
        "$$\n",
        "\\frac{d}{dx} x^n = nx^{n-1} \\quad \\text{for } n \\neq 0 \\tag{2.4.4}\n",
        "$$\n",
        "**Prove**:\n",
        "**a)** $n$ a positive integer\n",
        "\n",
        "Use the binomial expansion:\n",
        "$$\n",
        "(x+h)^n\n",
        "= \\sum_{k=0}^n \\binom nk x^{\\,n-k}h^k\n",
        "= x^n + n x^{\\,n-1}h + \\sum_{k=2}^n\\binom nk x^{\\,n-k}h^k.\n",
        "$$\n",
        "Then\n",
        "$$\n",
        "\\frac{(x+h)^n - x^n}{h}\n",
        "= n\\,x^{\\,n-1} + O(h),\n",
        "$$\n",
        "so taking $h\\to0$ gives\n",
        "$$\n",
        "\\frac{d}{dx}x^n = n\\,x^{\\,n-1}.\n",
        "$$\n",
        "\n",
        "**b)** $n$ a negative integer\n",
        "\n",
        "Write $n=-m$ with $m>0$. Then $x^n = 1/x^m$.  By the quotient (or chain) rule,\n",
        "$$\n",
        "\\frac{d}{dx}x^{-m}\n",
        "= -\\,x^{-m-1}\\,(m\\,x^{\\,m-1})\n",
        "= -m\\,x^{-(m+1)}\n",
        "= n\\,x^{\\,n-1}.\n",
        "$$\n",
        "**Example.**  \n",
        "Let $f(x)=x^5$. Then $f'(x)=5\\,x^4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "f = x**5\n",
        "sp.diff(f, x)\n",
        "# → 5*x**4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Exponential Rule\n",
        "$$\n",
        "\\frac{d}{dx} e^x = e^x \\tag{2.4.5}\n",
        "$$\n",
        "**Prove:**\n",
        "Via power series\n",
        "\n",
        "$$\n",
        "e^x = \\sum_{k=0}^\\infty \\frac{x^k}{k!}\n",
        "\\;\\implies\\;\n",
        "\\frac{d}{dx}e^x\n",
        "= \\sum_{k=1}^\\infty \\frac{k x^{k-1}}{k!}\n",
        "= \\sum_{j=0}^\\infty \\frac{x^j}{j!}\n",
        "= e^x.\n",
        "$$\n",
        "\n",
        "Via the limit definition of \\(e\\)\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}e^x\n",
        "=\\lim_{h\\to0}\\frac{e^{x+h}-e^x}{h}\n",
        "=e^x\\lim_{h\\to0}\\frac{e^h-1}{h}\n",
        "=e^x\\cdot1\n",
        "= e^x.\n",
        "$$\n",
        "\n",
        "**Example.**  \n",
        "Let $f(x)=e^x$. Then $f'(x)=e^x.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "f = sp.exp(x)\n",
        "sp.diff(f, x)\n",
        "# → exp(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Logarithm Rule\n",
        "$$\n",
        "\\frac{d}{dx} \\ln x = x^{-1} \\tag{2.4.6}\n",
        "$$\n",
        "**Prove**:\n",
        "Since $y = \\ln x$ is the inverse of $x = e^y$,\n",
        "$$\n",
        "1 = \\frac{d}{dx}(e^y)\n",
        "  = e^y\\frac{dy}{dx}\n",
        "  = x\\frac{dy}{dx}\n",
        "\\quad\\Longrightarrow\\quad\n",
        "\\frac{dy}{dx} = \\frac1x.\n",
        "$$\n",
        "Hence\n",
        "$$\n",
        "\\frac{d}{dx}\\ln x = \\frac1x.\n",
        "$$\n",
        "**Example.**  \n",
        "Let $f(x)=\\ln(x)$. Then $f'(x)=\\frac1x.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "f = sp.log(x)\n",
        "sp.diff(f, x)\n",
        "# → 1/x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Functions composed from differentiable functions are often themselves differentiable.  \n",
        "The following rules come in handy for working with compositions of any differentiable functions $ f $ and $ g $, and constant $ C $:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Constant Multiple Rule\n",
        "$$\n",
        "\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x) \\quad \\tag{2.4.7}\n",
        "$$\n",
        "**Prove:**\n",
        "For any constant $C$:\n",
        "$$\n",
        "\\frac{d}{dx}\\bigl[C\\,f(x)\\bigr]\n",
        "=\\lim_{h\\to0}\\frac{C\\,f(x+h)-C\\,f(x)}{h}\n",
        "=\\lim_{h\\to0}\\frac{C\\bigl[f(x+h)-f(x)\\bigr]}{h}\n",
        "=C\\;\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}\n",
        "=C\\,f'(x).\n",
        "$$\n",
        "**Example.**  \n",
        "Let $f(x)=5\\,x^3$.  Then by the power rule,  \n",
        "$$\n",
        "f'(x)=5\\cdot3\\,x^2 = 15\\,x^2.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "f = 5*x**3\n",
        "sp.diff(f, x)\n",
        "# → 15*x**2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Sum Rule\n",
        "$$\n",
        "\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x) \\quad  \\tag{2.4.8}\n",
        "$$\n",
        "**Prove:**\n",
        "For any two functions $f,g$:\n",
        "$$\n",
        "\\frac{d}{dx}\\bigl[f(x)+g(x)\\bigr]\n",
        "=\\lim_{h\\to0}\\frac{[f(x+h)+g(x+h)]-[f(x)+g(x)]}{h}\n",
        "=\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}\n",
        "  +\\lim_{h\\to0}\\frac{g(x+h)-g(x)}{h}\n",
        "=f'(x)+g'(x).\n",
        "$$\n",
        "**Example.**  \n",
        "Let $f(x)=x^2 + 3x$.  Then $f'(x) = 2x + 3.$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "f = x**2 + 3*x\n",
        "sp.diff(f, x)\n",
        "# → 2*x + 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Product Rule\n",
        "$$\n",
        "\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} g(x) + g(x) \\frac{d}{dx} f(x) \\quad \\tag{2.4.9}\n",
        "$$\n",
        "**Prove:**\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{d}{dx}\\bigl[f(x)g(x)\\bigr]\n",
        "&=\\lim_{h\\to0}\\frac{f(x+h)g(x+h)-f(x)g(x)}{h}\\\\\n",
        "&=\\lim_{h\\to0}\\frac{f(x+h)g(x+h)-f(x)g(x+h)+f(x)g(x+h)-f(x)g(x)}{h}\\\\\n",
        "&=\\lim_{h\\to0}\\frac{\\bigl[f(x+h)-f(x)\\bigr]\\,g(x+h)}{h}\n",
        "  +\\lim_{h\\to0}\\frac{f(x)\\,\\bigl[g(x+h)-g(x)\\bigr]}{h}\\\\\n",
        "&=\\Bigl(\\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}\\Bigr)\\,g(x)\n",
        "  +f(x)\\,\\Bigl(\\lim_{h\\to0}\\frac{g(x+h)-g(x)}{h}\\Bigr)\\\\\n",
        "&=f'(x)\\,g(x)+f(x)\\,g'(x).\n",
        "\\end{aligned}\n",
        "$$\n",
        "**Example.**  \n",
        "Let $h(x)=x^2\\sin x$.  Then $h'(x)=x^2\\cos x + 2x\\sin x.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "h = x**2 * sp.sin(x)\n",
        "sp.diff(h, x)\n",
        "# → x**2*cos(x) + 2*x*sin(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Quotient Rule\n",
        "$$\n",
        "\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) = \\frac{g(x) \\frac{d}{dx} f(x) - f(x) \\frac{d}{dx} g(x)}{g^2(x)} \\quad \\tag{2.4.10}\n",
        "$$\n",
        "**Prove:**\n",
        "Assume $g(x)\\neq0$.  Write\n",
        "$$\n",
        "\\frac{f(x)}{g(x)} = f(x)\\,\\bigl[g(x)\\bigr]^{-1}.\n",
        "$$\n",
        "Then by the product rule and the chain rule (derivative of $u^{-1}$ is $-u^{-2}u'$):\n",
        "$$\n",
        "\\frac{d}{dx}\\frac{f}{g}\n",
        "=\\frac{d}{dx}\\bigl(f\\cdot g^{-1}\\bigr)\n",
        "=f'\\,g^{-1}+f\\;\\bigl(-g^{-2}g'\\bigr)\n",
        "=\\frac{f'}{g}-\\frac{f\\,g'}{g^2}\n",
        "=\\frac{g\\,f' - f\\,g'}{g^2}.\n",
        "$$\n",
        "\n",
        "**Example.**  \n",
        "Let $q(x)=\\frac{x^2}{1+x}$.  Then  \n",
        "$$\n",
        "q'(x)\n",
        "= \\frac{(1+x)\\cdot2x - x^2\\cdot1}{(1+x)^2}\n",
        "= \\frac{2x+2x^2 - x^2}{(1+x)^2}\n",
        "= \\frac{x(2+x)}{(1+x)^2}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "q = x**2/(1+x)\n",
        "sp.diff(q, x)\n",
        "# → x*(x + 2)/(x + 1)**2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4.2. Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from matplotlib_inline import backend_inline\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saved in the d2l package for later use\n",
        "def use_svg_display():\n",
        "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
        "    backend_inline.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_figsize(figsize=(3.5, 2.5)):\n",
        "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
        "    use_svg_display()\n",
        "    d2l.plt.rcParams['figure.figsize'] = figsize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
        "         ylim=None, xscale='linear', yscale='linear',\n",
        "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
        "    \"\"\"Plot data points.\"\"\"\n",
        "\n",
        "    def has_one_axis(X):  # True if X (tensor or list) has 1 axis\n",
        "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
        "                and not hasattr(X[0], \"__len__\"))\n",
        "\n",
        "    if has_one_axis(X): X = [X]\n",
        "    if Y is None:\n",
        "        X, Y = [[]] * len(X), X\n",
        "    elif has_one_axis(Y):\n",
        "        Y = [Y]\n",
        "    if len(X) != len(Y):\n",
        "        X = X * len(Y)\n",
        "\n",
        "    set_figsize(figsize)\n",
        "    if axes is None:\n",
        "        axes = d2l.plt.gca()\n",
        "    axes.cla()\n",
        "    for x, y, fmt in zip(X, Y, fmts):\n",
        "        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)\n",
        "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.arange(0, 3, 0.1)\n",
        "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you can't run: I already run it, you can check it here: https://colab.research.google.com/drive/1-NpioIlXCycAxQMpy_SMytaofpnISf8J?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4.3 Partial Derivatives and Gradients\n",
        "\n",
        "Thus far, we have been differentiating functions of just one variable. In deep learning, we also need to work with functions of *many* variables. We briefly introduce notions of the derivative that apply to such *multivariate* functions.\n",
        "\n",
        "Let $y = f(x_1, x_2, \\dots, x_n)$ be a function with **n** variables. The *partial derivative* of y with respect to its $i^{th}$ parameter $x_i$ is\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i}\n",
        "\\;=\\;\n",
        "\\lim_{h \\to 0}\n",
        "\\frac{f(x_1,\\dots,x_{i-1},x_i + h,\\,x_{i+1},\\dots,x_n) \\;-\\; f(x_1,\\dots,x_i,\\dots,x_n)}{h}.\n",
        "\\tag{2.4.6}\n",
        "$$\n",
        "\n",
        "To calculate $\\frac{\\partial y}{\\partial x_i}$, we treat all other $x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_{n}$ as constants and calculate the derivative of $y$ with respect to $x_i$. The following notational conventions for partial derivatives are all common and all mean the same thing:\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i}\n",
        "=\n",
        "\\frac{\\partial f}{\\partial x_i}\n",
        "=\n",
        "\\partial_{x_i} f\n",
        "=\n",
        "\\partial_i f\n",
        "=\n",
        "f_{x_i}\n",
        "=\n",
        "f_i\n",
        "=\n",
        "D_i f\n",
        "=\n",
        "D_{x_i} f.\n",
        "\\tag{2.4.7}\n",
        "$$\n",
        "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain a vector that is called the gradient of the function. Suppose that the input of function $f:\\mathbb R^n\\to\\mathbb R$ is an $n$-dimensional vector $\\mathbf x=[x_1,\\dots,x_n]^T$ and the output is a scalar. The gradient of the function $f$ with respect to $x$ is a vector of $n$ partial derivatives:\n",
        "$$\n",
        "\\nabla_{\\mathbf x}f(\\mathbf x)\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\partial_{x_1}f(\\mathbf x)\\\\\n",
        "\\partial_{x_2}f(\\mathbf x)\\\\\n",
        "\\vdots\\\\\n",
        "\\partial_{x_n}f(\\mathbf x)\n",
        "\\end{bmatrix}.\n",
        "\\tag{2.4.8}\n",
        "$$\n",
        "\n",
        "When there is no ambiguity $\\nabla_{\\mathbf x} f(\\mathbf x)$ is typically replaced by $\\nabla f(\\mathbf x)$.\n",
        "\n",
        "The following rules come in handy for differentiating multivariate functions:\n",
        "\n",
        "- Rule 1: For all $\\mathbf A\\in\\mathbb R^{m\\times n}$ we have $\\nabla_{\\mathbf x}(\\mathbf A\\mathbf x)=\\mathbf A^T$ and $\\nabla_{\\mathbf x}(\\mathbf x^T \\mathbf A)=\\mathbf A$.\n",
        "\n",
        "We have:\n",
        "$$ \\mathbf A\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "A_{00}\\hspace{0.5em} A_{01}\\hspace{0.5em} \\dots \\hspace{0.5em} A_{0n} \\\\\n",
        "A_{10}\\hspace{0.5em} A_{11}\\hspace{0.5em} \\dots \\hspace{0.5em} A_{1n}\\\\\n",
        "\\vdots\\\\\n",
        "A_{m0}\\hspace{0.5em} A_{m1}\\hspace{0.5em} \\dots \\hspace{0.5em} A_{mn}\n",
        "\\end{bmatrix}\n",
        ".\\quad\n",
        "\\mathbf x\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "x_{0} \\\\\n",
        "x_{1}\\\\\n",
        "\\vdots\\\\\n",
        "x_{n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Then\n",
        "$$\\mathbf A\\mathbf x\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "A_{00}x_{0} + A_{01}x_{1} + \\dots + A_{0n}x_{n}\\\\\n",
        "A_{10}x_{0} + A_{11}x_{1} + \\dots + A_{1n}x_{n}\\\\\n",
        "\\vdots\\\\\n",
        "A_{n0}x_{0} + A_{n1}x_{1} + \\dots + A_{nn}x_{n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Set $f = \\mathbf A\\mathbf x$, then\n",
        "$$\n",
        "\\nabla_{\\mathbf x}f(\\mathbf x)\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\partial_{x_0}f_{0}(\\mathbf x) \\hspace{0.5em} \\partial_{x_1}f_{0}(\\mathbf x) \\hspace{0.5em} \\dots \\hspace{0.5em} \\partial_{x_n}f_{0}(\\mathbf x)\\\\\n",
        "\\partial_{x_0}f_{1}(\\mathbf x) \\hspace{0.5em} \\partial_{x_1}f_{1}(\\mathbf x) \\hspace{0.5em} \\dots \\hspace{0.5em} \\partial_{x_n}f_{1}(\\mathbf x)\\\\\n",
        "\\vdots\\\\\n",
        "\\partial_{x_0}f_{n}(\\mathbf x) \\hspace{0.5em} \\partial_{x_1}f_{n}(\\mathbf x) \\hspace{0.5em} \\dots \\hspace{0.5em} \\partial_{x_n}f_{n}(\\mathbf x)\n",
        "\\end{bmatrix} ^{T}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "A_{00}\\hspace{0.5em} A_{01}\\hspace{0.5em} \\dots \\hspace{0.5em} A_{0n} \\\\\n",
        "A_{10}\\hspace{0.5em} A_{11}\\hspace{0.5em} \\dots \\hspace{0.5em} A_{1n}\\\\\n",
        "\\vdots\\\\\n",
        "A_{m0}\\hspace{0.5em} A_{m1}\\hspace{0.5em} \\dots \\hspace{0.5em} A_{mn}\n",
        "\\end{bmatrix} ^{T}\n",
        "= \\mathbf A ^{T}\n",
        "$$\n",
        "So for all $\\mathbf A\\in\\mathbb R^{m\\times n}$, we have $$\\nabla_{\\mathbf x}(\\mathbf A\\mathbf x)=\\mathbf A^T.$$\n",
        "Similarly for all $\\mathbf A\\in\\mathbb R^{n\\times m}$, we have $$\\nabla_{\\mathbf x}(\\mathbf x^T \\mathbf A)=\\mathbf A.$$\n",
        "- Rule 2: For square matrices $\\mathbf A\\in\\mathbb R^{n\\times n}: \\nabla_{\\mathbf x}(\\mathbf x^T \\mathbf A\\mathbf x)=(\\mathbf A+\\mathbf A^T)\\mathbf x$, in particular $\\nabla_{\\mathbf x}\\|\\mathbf x\\|^2=2\\mathbf x$.\n",
        "\n",
        "Let $f(\\mathbf x) = \\mathbf x^T \\mathbf A\\mathbf x = \\sum_{i=1}^n \\sum_{j=1}^n x_i A_{ij} x_j$, then\n",
        "$$\n",
        "\\nabla_{\\mathbf x}f(\\mathbf x)\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\partial_{x_1}f(\\mathbf x)\\\\\n",
        "\\partial_{x_2}f(\\mathbf x)\\\\\n",
        "\\vdots\\\\\n",
        "\\partial_{x_n}f(\\mathbf x)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\sum_{j=1}^{n} \\frac{\\partial x_{1}A_{1j}x_{j}}{\\partial x_1} + \\sum_{i=1}^{n} \\frac{\\partial x_{i}A_{i1}x_{1}}{\\partial x_1}\\\\\n",
        "\\sum_{j=1}^{n} \\frac{\\partial x_{2}A_{2j}x_{j}}{\\partial x_2} + \\sum_{i=1}^{n} \\frac{\\partial x_{i}A_{i2}x_{2}}{\\partial x_2}\\\\\n",
        "\\vdots\\\\\n",
        "\\sum_{j=1}^{n} \\frac{\\partial x_{n}A_{nj}x_{j}}{\\partial x_n} + \\sum_{i=1}^{n} \\frac{\\partial x_{i}A_{in}x_{n}}{\\partial x_n}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\sum_{j=1}^{n} A_{1j}x_{j} + \\sum_{i=1}^{n} x_{i}A_{i1}\\\\\n",
        "\\sum_{j=1}^{n} A_{2j}x_{j} + \\sum_{i=1}^{n} x_{i}A_{i2}\\\\\n",
        "\\vdots\\\\\n",
        "\\sum_{j=1}^{n} A_{nj}x_{j} + \\sum_{i=1}^{n} x_{i}A_{in}\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf x}f(\\mathbf x)\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\sum_{j=1}^{n} A_{1j}x_{j}\\\\\n",
        "\\sum_{j=1}^{n} A_{2j}x_{j}\\\\\n",
        "\\vdots\\\\\n",
        "\\sum_{j=1}^{n} A_{nj}x_{j}\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "\\sum_{i=1}^{n} x_{i}A_{i1}\\\\\n",
        "\\sum_{i=1}^{n} x_{i}A_{i2}\\\\\n",
        "\\vdots\\\\\n",
        "\\sum_{i=1}^{n} x_{i}A_{in}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\mathbf A \\mathbf x + \\mathbf A^{T} \\mathbf x\n",
        "=\n",
        "(\\mathbf A + \\mathbf A^{T}) \\mathbf x.\n",
        "$$\n",
        "\n",
        "Similarly, for any matrix $\\mathbf X$, we have $\\|\\mathbf X\\|_F^2 = \\mathbf X^{T}\\mathbf X = \\mathbf X^{T}\\mathbf I\\mathbf X$, with $\\mathbf I$ is Identity matrix.\n",
        "\n",
        "Then, apply Rule 2, we have $\\|\\mathbf X\\|_F^2 = \\mathbf X^{T}\\mathbf I\\mathbf X = (\\mathbf I + \\mathbf I^{T})\\mathbf X = 2\\mathbf I\\mathbf X = 2\\mathbf X$\n",
        "\n",
        "So, $\\nabla_X\\|X\\|_F^2=2X.$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2.4.4. Chain Rule**\n",
        "\n",
        "### **4.1 Introduction to the Chain Rule**\n",
        "\n",
        "The Chain Rule is a mathematical tool used to compute the derivative of composite functions.  \n",
        "In deep learning, we deal with complex, nested functions across multiple layers. The Chain Rule enables us to compute gradients for optimization.\n",
        "\n",
        "There are two main cases:\n",
        "\n",
        "- **Single-variable functions**: $y = f(g(x))$\n",
        "- **Multivariable functions**: $y = f(\\mathbf{u})$, where $\\mathbf{u} = g(\\mathbf{x})$\n",
        "\n",
        "### **4.2 Chain Rule for Single-Variable Functions**\n",
        "\n",
        "**Formula:** If $y = f(u)$ and $u = g(x)$, and both $f$ and $g$ are differentiable at their respective points, then:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
        "$$\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "- $\\frac{dy}{dx}$: Rate of change of $y$ with respect to $x$\n",
        "- $\\frac{dy}{du}$: Rate of change of $y$ with respect to $u$\n",
        "- $\\frac{du}{dx}$: Rate of change of $u$ with respect to $x$\n",
        "\n",
        "The Chain Rule \"chains\" these derivatives together.\n",
        "\n",
        "**Example: Find the derivative of the function $y = \\sin(x^2)$.**\n",
        "\n",
        "**Step 1: Identify the composition**\n",
        "\n",
        "Let\n",
        "$$\n",
        "u = x^2, \\quad \\text{then} \\quad y = \\sin(u).\n",
        "$$\n",
        "\n",
        "**Step 2: Apply the Chain Rule**\n",
        "\n",
        "By the Chain Rule:\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
        "\n",
        "**Step 3: Compute each part**\n",
        "\n",
        "- $\\frac{dy}{du} = \\cos(u)$  \n",
        "- $\\frac{du}{dx} = 2x$\n",
        "\n",
        "**Step 4: Substitute back**\n",
        "\n",
        "$$\\frac{dy}{dx} = \\cos(u) \\cdot 2x = \\cos(x^2) \\cdot 2x$$\n",
        "\n",
        "### **4.3 Chain Rule for Multivariable Functions**\n",
        "\n",
        "**Formula:**  \n",
        "If $y = f(\\mathbf{u})$, where $\\mathbf{u} = (u_1, u_2, \\dots, u_m)$, and each $u_i = g_i(\\mathbf{x})$, with $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, then:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1} \\frac{\\partial u_1}{\\partial x_i} + \\frac{\\partial y}{\\partial u_2} \\frac{\\partial u_2}{\\partial x_i} + \\cdots + \\frac{\\partial y}{\\partial u_m} \\frac{\\partial u_m}{\\partial x_i}\n",
        "$$\n",
        "\n",
        "**When computing the derivative of $y$ with respect to each $x_i$, we apply the chain rule:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i} = \\sum_{j=1}^{m} \\frac{\\partial y}{\\partial u_j} \\cdot \\frac{\\partial u_j}{\\partial x_i}\n",
        "$$\n",
        "\n",
        "**In vector form:**\n",
        "\n",
        "Instead of handling one variable at a time, we can organize the derivatives into vectors and matrices:\n",
        "\n",
        "**Gradient with respect to $\\mathbf{x}$:**\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} y =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial y}{\\partial x_1} \\\\\n",
        "\\frac{\\partial y}{\\partial x_2} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial y}{\\partial x_n}\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^n\n",
        "$$\n",
        "\n",
        "**Gradient with respect to $\\mathbf{u}$:**\n",
        "$$\n",
        "\\nabla_{\\mathbf{u}} y =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial y}{\\partial u_1} \\\\\n",
        "\\frac{\\partial y}{\\partial u_2} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial y}{\\partial u_m}\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^m\n",
        "$$\n",
        "\n",
        "**Jacobian matrix $\\mathbf{A}$ (partial derivatives of $\\mathbf{u}$ with respect to $\\mathbf{x}$):**\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial u_1}{\\partial x_1} & \\frac{\\partial u_2}{\\partial x_1} & \\cdots & \\frac{\\partial u_m}{\\partial x_1} \\\\\n",
        "\\frac{\\partial u_1}{\\partial x_2} & \\frac{\\partial u_2}{\\partial x_2} & \\cdots & \\frac{\\partial u_m}{\\partial x_2} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial u_1}{\\partial x_n} & \\frac{\\partial u_2}{\\partial x_n} & \\cdots & \\frac{\\partial u_m}{\\partial x_n}\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^{n \\times m}\n",
        "$$\n",
        "\n",
        "**Final Vectorized Chain Rule:**\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} y = \\mathbf{A} \\nabla_{\\mathbf{u}} y\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ is a matrix containing the partial derivatives $\\frac{\\partial u_j}{\\partial x_i}$.  \n",
        "- $\\nabla_{\\mathbf{x}} y$: Gradient of $y$ with respect to $\\mathbf{x}$.  \n",
        "- $\\nabla_{\\mathbf{u}} y$: Gradient of $y$ with respect to $\\mathbf{u}$.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Suppose:\n",
        "\n",
        "- $y = u_1^2 + u_2^2$  \n",
        "- $u_1 = x_1 + x_2$, $u_2 = x_1 - x_2$\n",
        "\n",
        "Compute $\\frac{\\partial y}{\\partial x_1}$ and $\\frac{\\partial y}{\\partial x_2}$.\n",
        "\n",
        "**Step 1: Compute the partial derivatives**\n",
        "\n",
        "- $\\frac{\\partial y}{\\partial u_1} = 2u_1$  \n",
        "- $\\frac{\\partial y}{\\partial u_2} = 2u_2$  \n",
        "- $\\frac{\\partial u_1}{\\partial x_1} = 1$, $\\frac{\\partial u_1}{\\partial x_2} = 1$  \n",
        "- $\\frac{\\partial u_2}{\\partial x_1} = 1$, $\\frac{\\partial u_2}{\\partial x_2} = -1$\n",
        "\n",
        "**Step 2: Apply the Chain Rule**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial x_1} + \\frac{\\partial y}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial x_1} = 2u_1 \\cdot 1 + 2u_2 \\cdot 1 = 2u_1 + 2u_2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_2} = \\frac{\\partial y}{\\partial u_1} \\cdot \\frac{\\partial u_1}{\\partial x_2} + \\frac{\\partial y}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial x_2} = 2u_1 \\cdot 1 + 2u_2 \\cdot (-1) = 2u_1 - 2u_2\n",
        "$$\n",
        "\n",
        "**Step 3: Substitute $u_1 = x_1 + x_2$, $u_2 = x_1 - x_2$**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_1} = 2(x_1 + x_2) + 2(x_1 - x_2) = 4x_1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_2} = 2(x_1 + x_2) - 2(x_1 - x_2) = 4x_2\n",
        "$$\n",
        "\n",
        "\n",
        "**Vector Form:**\n",
        "\n",
        "- Gradient: $\\nabla_{\\mathbf{x}} y = \\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \\\\ \\frac{\\partial y}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 4x_1 \\\\ 4x_2 \\end{bmatrix}$  \n",
        "- Jacobian Matrix: $\\mathbf{A} = \\begin{bmatrix} \\frac{\\partial u_1}{\\partial x_1} & \\frac{\\partial u_1}{\\partial x_2} \\\\ \\frac{\\partial u_2}{\\partial x_1} & \\frac{\\partial u_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}$  \n",
        "- Gradient w.r.t. $\\mathbf{u}$: $\\nabla_{\\mathbf{u}} y = \\begin{bmatrix} 2u_1 \\\\ 2u_2 \\end{bmatrix}$\n",
        "\n",
        "**Verification:**\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} y = \\mathbf{A} \\nabla_{\\mathbf{u}} y = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} 2u_1 \\\\ 2u_2 \\end{bmatrix} = \\begin{bmatrix} 2u_1 + 2u_2 \\\\ 2u_1 - 2u_2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Substitute $u_1 = x_1 + x_2$, $u_2 = x_1 - x_2$ to get:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{x}} y = \\begin{bmatrix} 4x_1 \\\\ 4x_2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This matches our earlier result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
