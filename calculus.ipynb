{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzcx4HcnGwux"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Partial Derivatives and Gradients**  \n",
        "-------------------------------------\n",
        "\n",
        "Thus far, we have been differentiating functions of just one variable. In deep learning, we also need to work with functions of *many* variables. We briefly introduce notions of the derivative that apply to such *multivariate* functions.\n",
        "\n",
        "Let $y = f(x_1, x_2, \\dots, x_n)$ be a function with $n$ variables. The *partial derivative* of y with respect to its $i^{th}$ parameter $x_i$ is\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i}\n",
        "\\;=\\;\n",
        "\\lim_{h \\to 0}\n",
        "\\frac{f(x_1,\\dots,x_{i-1},x_i + h,\\,x_{i+1},\\dots,x_n) \\;-\\; f(x_1,\\dots,x_i,\\dots,x_n)}{h}.\n",
        "\\tag{2.4.6}\n",
        "$$\n",
        "\n",
        "To calculate $\\frac{\\partial y}{\\partial x_i}$, we treat all other $x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_{n}$ as constants and calculate the derivative of $y$ with respect to $x_i$. The following notational conventions for partial derivatives are all common and all mean the same thing:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x_i}\n",
        "=\n",
        "\\frac{\\partial f}{\\partial x_i}\n",
        "=\n",
        "\\partial_{x_i} f\n",
        "=\n",
        "\\partial_i f\n",
        "=\n",
        "f_{x_i}\n",
        "=\n",
        "f_i\n",
        "=\n",
        "D_i f\n",
        "=\n",
        "D_{x_i} f.\n",
        "\\tag{2.4.7}\n",
        "$$\n",
        "\n",
        "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain a vector that is called the gradient of the function. Suppose that the input of function $f:\\mathbb R^n\\to\\mathbb R$ is an $n$-dimensional vector $\\mathbf x=[x_1,\\dots,x_n]^T$ and the output is a scalar. The gradient of the function $f$ with respect to **x** is a vector of $n$ partial derivatives:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf x}f(\\mathbf x)\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\partial_{x_1}f(\\mathbf x)\\\\\n",
        "\\partial_{x_2}f(\\mathbf x)\\\\\n",
        "\\vdots\\\\\n",
        "\\partial_{x_n}f(\\mathbf x)\n",
        "\\end{bmatrix}.\n",
        "\\tag{2.4.8}\n",
        "$$\n",
        "\n",
        "When there is no ambiguity $\\nabla_{\\mathbf x} f(\\mathbf x)$ is typically replaced by $\\nabla f(\\mathbf x)$. The following rules come in handy for differentiating multivariate functions:\n",
        "\n",
        "- For all $A\\in\\mathbb R^{m\\times n}$ we have $\\nabla_{\\mathbf x}(A\\mathbf x)=A^T$ and $\\nabla_{\\mathbf x}(\\mathbf x^T A)=A$.  \n",
        "- For square matrices $A\\in\\mathbb R^{n\\times n}: \\nabla_{\\mathbf x}(\\mathbf x^T A\\mathbf x)=(A+A^T)\\mathbf x$, in particular $\\nabla_{\\mathbf x}\\|\\mathbf x\\|^2=2\\mathbf x$.\n",
        "\n",
        "Similarly, for any matrix $X$, we have\n",
        "\n",
        "$$\n",
        "\\nabla_X\\|X\\|_F^2=2X.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
